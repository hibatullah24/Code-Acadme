{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25e68e48-f76e-4b8b-9c99-9b531ea5e72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\user pc\\appdata\\roaming\\python\\python312\\site-packages (4.33.0)\n",
      "Requirement already satisfied: urllib3~=2.4.0 in c:\\users\\user pc\\appdata\\roaming\\python\\python312\\site-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: trio~=0.30.0 in c:\\users\\user pc\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in c:\\users\\user pc\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.4.26 in c:\\users\\user pc\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (2025.4.26)\n",
      "Requirement already satisfied: typing_extensions~=4.13.2 in c:\\users\\user pc\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (4.13.2)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\user pc\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\user pc\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\user pc\\appdata\\roaming\\python\\python312\\site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50dbdfd0-2145-4113-b14c-2644bd755029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c89616-9cf6-4048-ad8a-c13bd407bf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\user pc\\appdata\\roaming\\python\\python312\\site-packages (4.33.0)\n",
      "Requirement already satisfied: webdriver_manager in c:\\users\\user pc\\appdata\\roaming\\python\\python312\\site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3~=2.4.0 in c:\\users\\user pc\\appdata\\roaming\\python\\python312\\site-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: trio~=0.30.0 in c:\\users\\user pc\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in c:\\users\\user pc\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.4.26 in c:\\users\\user pc\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (2025.4.26)\n",
      "Requirement already satisfied: typing_extensions~=4.13.2 in c:\\users\\user pc\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (4.13.2)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver_manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver_manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver_manager) (24.1)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\user pc\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\user pc\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\user pc\\appdata\\roaming\\python\\python312\\site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (3.3.2)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "142f97e0-be0e-4cb9-b40c-a850246bac08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University of Technology and Applied Sciences\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "# Example usage\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "# Your scraping logic here\n",
    "driver.get('https://www.utas.edu.om/Home')\n",
    "print(driver.title)\n",
    "# Wait for 2 seconds\n",
    "time.sleep(2)\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dd93d31-a09c-487c-be5d-789d2199a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "drv = webdriver.Chrome()\n",
    "drv.get(\"https://www.google.com\")\n",
    "\n",
    "box = drv.find_element(By.NAME, \"q\")\n",
    "box.send_keys(\"UTAS Muscat\", Keys.RETURN)\n",
    "\n",
    "time.sleep(30)\n",
    "drv.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37388ada-ba84-4b03-b981-55a7137a2cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "import time\n",
    "\n",
    "# Set up the Firefox driver using GeckoDriverManager\n",
    "service = Service(GeckoDriverManager().install())\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "# Your scraping logic here\n",
    "driver.get('https://www.google.com/')\n",
    "print(driver.title)\n",
    "\n",
    "# Wait for 2 seconds\n",
    "time.sleep(2)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "300f9b19-8d0e-44b0-bc44-968dd6aecc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete. Data saved to 'BookStore1.csv'.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome() \n",
    "\n",
    "driver.get(\"https://books.toscrape.com/catalogue/page-1.html\")\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "books = driver.find_elements(By.CLASS_NAME, \"product_pod\")\n",
    "\n",
    "# Prepare CSV file\n",
    "with open('BookStore1.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Price', 'Availability', 'Star Rating'])\n",
    "\n",
    "    for book in books:\n",
    "        # Title\n",
    "        title = book.find_element(By.TAG_NAME, \"h3\").find_element(By.TAG_NAME, \"a\").get_attribute(\"title\")\n",
    "\n",
    "        # Price\n",
    "        price = book.find_element(By.CLASS_NAME, \"price_color\").text.strip()\n",
    "\n",
    "        # Availability\n",
    "        availability = book.find_element(By.CLASS_NAME, \"availability\").text.strip()\n",
    "\n",
    "        # Star Rating (e.g., class=\"star-rating Three\")\n",
    "        star_class = book.find_element(By.CSS_SELECTOR, \"p.star-rating\").get_attribute(\"class\")\n",
    "        star_rating = star_class.split()[-1]  # Gets the rating word like \"Three\"\n",
    "\n",
    "        writer.writerow([title, price, availability, star_rating])\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "print(\"Scraping complete. Data saved to 'BookStore1.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0ffbf30-26aa-4107-95b4-48943e2a0a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total books: 20\n",
      "Average price: £ 38.05\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://books.toscrape.com/catalogue/page-1.html\")\n",
    "books = driver.find_elements(By.CLASS_NAME, \"product_pod\")\n",
    "\n",
    "\n",
    "file = open('BookStore1.csv', 'w', newline='', encoding='utf-8')\n",
    "writer = csv.writer(file)\n",
    "writer.writerow(['Title', 'Price', 'Availability', 'Star Rating'])\n",
    "\n",
    "\n",
    "total_books = 0\n",
    "total_price = 0.0\n",
    "\n",
    "\n",
    "for book in books:\n",
    "    title = book.find_element(By.XPATH, \".//h3/a\").get_attribute(\"title\")\n",
    "    price_text = book.find_element(By.CLASS_NAME, \"price_color\").text.strip()  \n",
    "    price = float(price_text[1:])  # remove £ and convert to float\n",
    "    availability = book.find_element(By.CLASS_NAME, \"availability\").text.strip()\n",
    "    star = book.find_element(By.CSS_SELECTOR, \"p.star-rating\").get_attribute(\"class\").split()[-1]\n",
    "\n",
    "    writer.writerow([title, price_text, availability, star])\n",
    "\n",
    "    # Update totals\n",
    "    total_books += 1\n",
    "    total_price += price\n",
    "\n",
    "\n",
    "file.close()\n",
    "driver.quit()\n",
    "print(\"Total books:\", total_books)\n",
    "print(\"Average price: £\", round(total_price / total_books, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd179011-6350-43f9-aa50-3a59163642cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .txt , then class\n",
    "#xpath if you will use loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec285a47-1d7c-4467-bc26-d620a8e0d3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Page loaded.\n",
      "✅ Article data saved to bbc_single_article.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"start-maximized\")\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0\")\n",
    "\n",
    "# Start the WebDriver\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "url = \"https://www.bbc.com/news/articles/ckg4zx0x778o\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the headline to appear\n",
    "try:\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//h1'))\n",
    "    )\n",
    "    print(\"✅ Page loaded.\")\n",
    "except:\n",
    "    print(\"❌ Page did not load properly.\")\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "# Extract headline\n",
    "headline = driver.find_element(By.XPATH, '//h1').text.strip()\n",
    "\n",
    "# Extract summary: first <p> inside the main article section\n",
    "try:\n",
    "    summary = driver.find_element(By.XPATH, '//main//article//p').text.strip()\n",
    "except:\n",
    "    summary = \"\"\n",
    "\n",
    "# Save to JSON\n",
    "data = {\n",
    "    \"Headline\": headline,\n",
    "    \"URL\": url,\n",
    "    \"Summary\": summary\n",
    "}\n",
    "\n",
    "with open(\"bbc_single_article.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "driver.quit()\n",
    "print(\"✅ Article data saved to bbc_single_article.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9130d3ed-4b7f-4083-82ec-66f66c6beaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 38 unique article links after scrolling.\n",
      "✅ Scraped 38 articles successfully.\n",
      "[\n",
      "    {\n",
      "        \"Headline\": \"Atsuko Okatsuka's guide to Los Angeles\",\n",
      "        \"URL\": \"https://www.bbc.com/travel/article/20250527-atsuko-okatsukas-guide-to-los-angeles\",\n",
      "        \"Summary\": \"The quirky Drop Challenge comedian finds community in multicultural LA. Here are her top ways to enjoy the city, from catching live comedy to getting hot pot with her grandmother.\",\n",
      "        \"Image URL\": \"https://ichef.bbci.co.uk/images/ic/480xn/p0ldp88t.jpg.webp\"\n",
      "    },\n",
      "    {\n",
      "        \"Headline\": \"The return of Mexico's famous Tequila Express train\",\n",
      "        \"URL\": \"https://www.bbc.com/travel/article/20250523-the-return-of-mexicos-famous-tequila-express-train\",\n",
      "        \"Summary\": \"With \\\"tequila tourism\\\" gaining popularity in Mexico, a train taking tourists to the home of the spirit has relaunched after nine years away.\",\n",
      "        \"Image URL\": \"https://ichef.bbci.co.uk/images/ic/480xn/p0ld2h9g.jpg.webp\"\n",
      "    },\n",
      "    {\n",
      "        \"Headline\": \"The 25 best places to travel in 2025\",\n",
      "        \"URL\": \"https://www.bbc.com/travel/article/20250115-the-25-best-places-to-travel-in-2025\",\n",
      "        \"Summary\": \"From bubblegum-coloured lakes in Australia to a solar-powered safari camp in Botswana, these are BBC journalists' top destinations this year.\",\n",
      "        \"Image URL\": \"https://ichef.bbci.co.uk/images/ic/480xn/p0kjs566.jpg.webp\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Setup Chrome options for headless browsing\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  \n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "start_url = \"https://www.bbc.com/travel\"\n",
    "driver.get(start_url)\n",
    "time.sleep(3) \n",
    "\n",
    "SCROLL_PAUSE_TIME = 2\n",
    "NUM_SCROLLS = 10\n",
    "\n",
    "for _ in range(NUM_SCROLLS):\n",
    "    driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "article_elements = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"/travel/article/\"]')\n",
    "article_links = []\n",
    "seen_links = set()\n",
    "\n",
    "for elem in article_elements:\n",
    "    href = elem.get_attribute(\"href\")\n",
    "    if href and href not in seen_links:\n",
    "        seen_links.add(href)\n",
    "        article_links.append(href)\n",
    "\n",
    "print(f\"Collected {len(article_links)} unique article links after scrolling.\")\n",
    "\n",
    "\n",
    "articles_data = []\n",
    "\n",
    "for url in article_links:\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  \n",
    "\n",
    "    try:\n",
    "        headline = driver.find_element(By.TAG_NAME, \"h1\").text.strip()\n",
    "    except:\n",
    "        headline = \"Headline not available\"\n",
    "\n",
    "    try:\n",
    "        summary = driver.find_element(By.XPATH, \"//meta[@name='description']\").get_attribute(\"content\").strip()\n",
    "    except:\n",
    "        try:\n",
    "            summary = driver.find_element(By.CSS_SELECTOR, \"article p\").text.strip()\n",
    "        except:\n",
    "            summary = \"Summary not available\"\n",
    "\n",
    "    try:\n",
    "        image_element = driver.find_element(By.CSS_SELECTOR, \"#main-content article figure img\")\n",
    "        image_url = image_element.get_attribute(\"src\")\n",
    "    except:\n",
    "        image_url = \"Image not available\"\n",
    "\n",
    "    articles_data.append({\n",
    "        \"Headline\": headline,\n",
    "        \"URL\": url,\n",
    "        \"Summary\": summary,\n",
    "        \"Image URL\": image_url\n",
    "    })\n",
    "\n",
    "with open(\"bbc_travel_articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(articles_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Scraped {len(articles_data)} articles successfully.\")\n",
    "print(json.dumps(articles_data[:3], indent=4, ensure_ascii=False))  # preview first 3\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726525d9-c571-49a6-8d9d-5862485025c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d9e32-566c-4041-b57a-7d9f3c376360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe2b573-ae94-46b5-b488-0bb22335ef58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b3194-ba32-4fb6-bfd2-0e328f740ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394fc9e1-5f0c-4884-b490-bfe1a039bdae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
